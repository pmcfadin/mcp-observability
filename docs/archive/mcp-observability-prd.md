# Product Requirements Document â€“ **MCPâ€¯Observability**

*Version 1.0 â€¢ 15â€¯Junâ€¯2025*

---

## 1 Â· Executive Summary

*MCPâ€¯Observability* is a reusable, cloudâ€‘native observability bundle (Prometheus + Grafana + Loki + Tempo/Jaeger) augmented by a thin **MCP Server** that exposes AIâ€‘friendly query endpoints. One Helm chart *or* one Dockerâ€‘Compose file delivers fullâ€‘stack ingestion (logs, metrics, traces) **and** a secure, single API for downstream AI agents and developers.

---

## 2 Â· Goals & Nonâ€‘Goals

| Goals                                                                                                                                                                                                                            | Out of scope                                                                                |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |
| â€¢ Turnâ€‘key ingestion via **OpenTelemetry**.  \| â€¢ Single **MCP API** surfacing curated queries (logs, metrics, traces).  \| â€¢ Identical experience in Docker Compose and Helm/Kubernetes. \| â€¢ Secure by default (token / mTLS). | â€¢ Building new AI "insight" logic inside the stack.  \| â€¢ SaaS or proprietary dependencies. |

---

## 3 Â· Highâ€‘Level Architecture

```mermaid
flowchart TD
  subgraph Ingest
    OTLPg["OTLP gRPC\n:4317"]
    OTLPhttp["OTLP HTTP\n:4318"]
    Filelog["Filelog\nReceiver"]
  end
  OTEL["OpenTelemetry\nCollector"]
  Ingest --> OTEL
  subgraph Storage
    Prom["Prometheus\n:9090"]
    Loki["Loki\n:3100"]
    Tempo["Tempo/Jaeger\n:4318"]
    Grafana["Grafana\n:3000"]
  end
  OTEL --> Prom
  OTEL --> Loki
  OTEL --> Tempo
  Prom -- query --> MCP["MCP Server"]
  Loki -- query --> MCP
  Tempo -- query --> MCP
  Grafana -- dashboards/alerts --> MCP
  AI["AI Agents / CLI"] --> MCP
```

---

## 4 Â· Key Components

| Component          | Responsibility                                                                  | Container / Chart                      |
| ------------------ | ------------------------------------------------------------------------------- | -------------------------------------- |
| **otelâ€‘collector** | Intake OTLP gRPC/HTTP, container stdout (filelog).  Exports to Prom/Loki/Tempo. | `otel/opentelemetry-collector-contrib` |
| **Prometheus**     | Metrics store & Alertmanager.                                                   | `prom/prometheus`                      |
| **Loki**           | Log store / LogQL API.                                                          | `grafana/loki`                         |
| **Tempo**          | Distributed traces.                                                             | `grafana/tempo`                        |
| **Grafana**        | Dashboards, alerting (LLM App optional).                                        | `grafana/grafana-oss`                  |
| **MCP Server**     | Stateless query hub (REST / gRPC).                                              | `docker.io/pmcfadin/mcp-observability` |

---

## 5 Â· Functional Requirements

| ID   | Description                                                           | Priority |
| ---- | --------------------------------------------------------------------- | -------- |
| FRâ€‘1 | **Ingest** all logs/metrics/traces via OTLP and filelog.              | P0       |
| FRâ€‘2 | `GET /logs/errors?service&limit` â†’ last *n* error lines.              | P0       |
| FRâ€‘3 | `GET /metrics/latency?p=0.95&service&range` â†’ latency series.         | P0       |
| FRâ€‘4 | `GET /trace/{trace_id}` â†’ full trace JSON.                            | P1       |
| FRâ€‘5 | Token or mTLS auth for all MCP endpoints.                             | P0       |
| FRâ€‘6 | `helm install mcp-obs` deploys full stack on K8s.                     | P0       |
| FRâ€‘7 | `docker-compose -f mcp-obs.yml up -d` boots services locally.         | P0       |
| FRâ€‘8 | Preâ€‘bundled Grafana dashboards (IDs 16110, 13175, custom *ErrorOps*). | P1       |

---

## 6 Â· Nonâ€‘Functional Requirements

* **Performance** â€“ MCP API median â‰¤ 150 ms for 7â€‘day window queries.
* **Security** â€“ intraâ€‘cluster traffic only; external ports minimal.
* **Extensibility** â€“ new MCP endpoint â‰¤ 15 LoC & documented.
* **Observability** â€“ MCP Server exports its own OTLP telemetry.

---

## 7 Â· Useâ€‘Case Catalogue

1. **AI test agent** â†’ "new `ConnectionTimeout` errors in `checkout-service` last 30 min."
2. **Perf bot** polls latency endpoint, autoâ€‘scales if p95 > SLA.
3. **Developer** pastes traceâ€‘ID â†’ correlated logs via `/logs/context?trace_id=`.

---

## 8 Â· Deployment & Configuration Highlights

### Helm values (excerpt)

```yaml
mcpServer:
  image: docker.io/pmcfadin/mcp-observability:1.0.0
  env:
    LOKI_URL: http://loki:3100
    PROM_URL: http://prometheus:9090
    TEMPO_URL: http://tempo:4318
    AUTH_TOKEN: ${MCP_TOKEN}
otelCollector:
  receivers:
    otlp:
      protocols: {grpc: {}, http: {}}
    filelog:
      include: [/var/log/containers/*.log]
```

### Compose patch (service)

```yaml
mcp-server:
  image: docker.io/pmcfadin/mcp-observability:1.0.0
  environment:
    - LOKI_URL=http://loki:3100
    - PROM_URL=http://prometheus:9090
    - TEMPO_URL=http://tempo:4318
    - AUTH_TOKEN=${MCP_TOKEN}
  ports:
    - "8081:8081"  # REST
```

---

| Date        | Deliverable                                          |
| ----------- | ---------------------------------------------------- |
| **T + 2 w** | PoC MCP Server (logs endpoint) under Compose.        |
| **T + 4 w** | Helm chart v0.1 (logs + metrics) & docs.             |
| **T + 6 w** | Metrics/Traces endpoints, Auth, ErrorOps dashboard.  |
| **T + 8 w** | First production install, loadâ€‘test, SLA validation. |

---

## 10 Â· Risks & Mitigations

| Risk                           | Impact                     | Mitigation                                          |
| ------------------------------ | -------------------------- | --------------------------------------------------- |
| Loki label cardinality blowâ€‘up | Slow queries, storage cost | Strict label schema; bodyâ€‘only dynamic fields       |
| Helm vs Compose drift          | Maintenance burden         | Generate Compose via `helm template` CI step        |
| MCP bottleneck                 | Latency & availability     | Stateless design + horizontal scaling + query cache |

---

## 11 Â· Success Metrics

* **< 5 min** to first successful install in a new repo.
* **â‰¥ 90 %** of common queries answered via MCP (no direct Grafana/Loki).
* **p95 API latency < 150 ms** with 7â€‘day hot data.

---

## 12 Â· MCP Tool Endpoints

| Tool                          | Endpoint (REST)           | Method | Parameters / Notes                                                                      |
| ----------------------------- | ------------------------- | ------ | --------------------------------------------------------------------------------------- |
| **Fetch recent errors**       | `/logs/errors`            | GET    | `service`, `level`, `limit`, `range` â€“ returns last *n* lines sorted by timestamp       |
| **Search logs (regex/text)**  | `/logs/search`            | POST   | JSON body: `{query, service, range}` â€“ arbitrary LogQL compiled query                   |
| **Latency percentiles**       | `/metrics/latency`        | GET    | `service`, `percentile` (e.g. 0.95), `range` â€“ Prometheus histogram_quantile on demand |
| **Custom metric snapshot**    | `/metrics/query`          | POST   | Body: PromQL string â€“ returns value series                                              |
| **Trace lookup**              | `/traces/{trace_id}`      | GET    | Path param: `trace_id` â€“ raw Tempo/Jaeger trace JSON                                    |
| **Correlated logs for trace** | `/traces/{trace_id}/logs` | GET    | Uses span IDs to fetch matching log lines                                               |
| **Alert status**              | `/alerts`                 | GET    | Lists active Alertmanager alerts (JSON)                                                 |
| **Health check**              | `/health`                 | GET    | MCP server liveness & dependencies                                                      |

---

## 13 Â· User Story â€“ *AI Agent "RetroBot"* ðŸ“–

> **Persona:** RetroBot, an autonomous testâ€‘automation agent that validates every PR for Retro Swap Meet.

### Scenario 1 â€“ Diagnose failing endâ€‘toâ€‘end test

1. PR pipeline triggers RetroBot.
2. Endâ€‘toâ€‘end test fails with HTTP 500.
3. RetroBot calls `GET /logs/errors?service=checkout-service&limit=10&range=15m`.
4. MCP returns JSON with stackâ€‘trace lines; RetroBot detects `ConnectionTimeout`.
5. RetroBot requests correlated trace: `GET /traces/{trace_id}` using ID from log.
6. With span data showing DB latency, RetroBot posts a GitHub comment suggesting increasing connection pool.

### Scenario 2 â€“ Verify SLA after deployment

1. RetroBot waits 10 min postâ€‘deploy.
2. Calls `GET /metrics/latency?service=api-gateway&percentile=0.95&range=10m`.
3. Receives p95 = 280 ms (below 300 ms target) â†’ marks check âœ….
4. If value > 300 ms, RetroBot triggers `/alerts` to confirm no alert suppression, then pages DevOps.

### Scenario 3 â€“ Regression hunting with custom query

1. During canary rollout, RetroBot posts PromQL to `/metrics/query`:

   ```json
   {
     "query": "rate(http_requests_total{service=\"image-resizer\",status=~\"5..\"}[5m])"
   }
   ```
2. Detects spike; automatically calls `/logs/search` with regex "OutOfMemoryError".
3. Generates ticket with offending pod names extracted from log lines.

---

*---

## 14 Â· MCP Compliance Matrix

| MCP Concept    | Implementation in MCP Observability                                                                                                                                                                                                                                                |
| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Resources**  | Expose log, metric, and trace datasets via REST URIs `/resources/{type}`; each includes metadata (`schema`, `updated_at`) so MCP clients can include them as context.                                                                                                              |
| **Prompts**    | Bundl([modelcontextprotocol.io](https://modelcontextprotocol.io/docs/concepts/resources?utm_source=chatgpt.com))emplates (e.g., `diagnose_error`, `compare_latency`) served at `/prompts` per MCP spec, allowing clients to inject standardized instructions into LLM calls.       |
| **Tools**      | Endpoints in Â§12 are surfa([modelcontextprotocol.io](https://modelcontextprotocol.io/docs/concepts/prompts?utm_source=chatgpt.com))so LLMs can invoke actions (fetch errors, traces, metrics). Tools follow the MCP JSON schema for tool definitions.                              |
| **Sampling**   | MCP Server supports `/sample` endpoint wher([modelcontextprotocol.io](https://modelcontextprotocol.io/docs/concepts/tools?utm_source=chatgpt.com))odel completions; server signs the request, routes via MCP "sampling" flow to allow onâ€‘theâ€‘fly rootâ€‘cause analysis by AI agents. |
| **Roots**      | Deploymentâ€‘specific "roots" (e.g., `k8s://namespace/retro`, \`fi([modelcontextprotocol.io](https://modelcontextprotocol.io/docs/concepts/sampling?utm_source=chatgpt.com)) declared in manifest so clients know valid resource scopes.                                             |
| **Transports** | Primary transport is HTTP/2 (REST+JSON); optional gRPC stream for highâ€‘frequenc([modelcontextprotocol.io](https://modelcontextprotocol.io/docs/concepts/roots?utm_source=chatgpt.com)) MCP transport guidelines for idempotency and auth.                                          |

### Python Implementation Notes

* MCP Server written in **Python 3.12** using *FastAPI* for REST and *grpcio* for ([modelcontextprotocol.io](https://modelcontextprotocol.io/docs/concepts/architecture?utm_source=chatgpt.com))port.
* Shared Pydantic models enforce MCP JSON schemas for Resources, Prompts, Tools, and Sampling requests.
* Async httpx client used to query Loki/Prometheus/Tempo; results marshalled into MCP response objects.
* Unit tests with **pytest** and contract tests against cyanheads/mcp-resources reference suite.

---

Last updated: 15 Jun 2025*
